{"paragraphs":[{"text":"%md\n<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 10px\">\n<h3>LDA + Aspect Based Sentiment Analysis on Amazon Food Review Dataset</h3>\n<h4>Components Used</h4>\n* Spark version 1.6.1 for Hadoop 2.6</br>\n* Zeppelin version 0.6.0 with `SPARK_HOME` set to Spark 1.6.1 for Hadoop 2.6</br>\n* PHEMI Data Source Library 1.1</br>\n* PHEMI Zeppelin Library 1.0</br>\n* Stanford CoreNLP 3.8.0</br>\n</div>\n\n<div class=\"clearfix\" style=\"padding: 10px; padding-left: 0px\">\n<a href=\"http://www.phemi.com\"><img src=\"http://phemi.com/wp-content/uploads/2014/11/PHEMI-logo.png\" width=\"150px\" class=\"pull-left\" style=\"display: inline-block; margin: 0px;\"></a>\n</div>","dateUpdated":"2017-08-28T22:08:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1503958083569_-1578838409","id":"20170817-171806_1700861632","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14529"},{"title":"Load Data Collection","text":"import com.phemi.zeppelin.ZeppelinPhemiCredentials\nimport com.phemi.spark.PhemiSparkLib\nimport org.apache.spark.sql.functions.lit\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{StructType, LongType, StructField}\n\nval cred = new ZeppelinPhemiCredentials(z.getInterpreterContext())\nval psl = new PhemiSparkLib(cred, sqlContext)\n\n\ndef addColumnIndex(df: org.apache.spark.sql.DataFrame) = sqlContext.createDataFrame(\n  // Add Column index\n  df.rdd.zipWithIndex.map{case (row, columnindex) => Row.fromSeq(row.toSeq :+ columnindex)},\n  // Create schema\n  StructType(df.schema.fields :+ StructField(\"Doc_Id\", LongType, false))\n)\n\n// repartition\nvar df1 = psl.loadPhemiDataCollection(\"Food Reviews\").repartition(20)\n\n// remove duplicate reviews and add row number\nvar df_FoodReview = addColumnIndex(df1.select(\"Text\").distinct).cache\nz.show(df_FoodReview.select(\"Doc_Id\", \"Text\"))","dateUpdated":"2017-08-28T22:10:09+0000","config":{"colWidth":9,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":289,"optionOpen":false,"keys":[],"values":[{"name":"Text","index":1,"aggr":"sum"}],"groups":[],"scatter":{"yAxis":{"name":"Text","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1503958083570_-1577684162","id":"20170817-164102_1830385222","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14530"},{"title":"Get Count","text":"df_FoodReview.count()","dateUpdated":"2017-08-28T22:08:03+0000","config":{"colWidth":3,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":453,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1503958083586_-315707770","id":"20170817-183056_1484484721","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14531"},{"title":"Pre - processing (Clean, Lemmatization, POS, Stop Words Removal)","text":"import edu.stanford.nlp.pipeline._\nimport edu.stanford.nlp.ling.CoreAnnotations._\nimport scala.collection.JavaConversions._\nimport java.util.Properties\nimport scala.collection.mutable._\nimport org.apache.spark.sql.types.{StringType}\nimport org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer, StopWordsRemover}\n\n// covert dataframe to RDD\nval rddTextWithHTML = df_FoodReview.select(\"Text\").map(x => x.toString)\n\n// remove HTML tags\nval rddText = rddTextWithHTML.map(_.replaceAll(\"\\\\<.*?>\",\"\"))\n\n// lemmatization - CoreNLP\nval rddLemma = rddText.map{text =>       \n    \n    val props = new Properties()\n    props.put(\"annotators\", \"tokenize, ssplit, pos, lemma\")\n    val pipeline = new StanfordCoreNLP(props)\n    val doc = new Annotation(text)\n    pipeline.annotate(doc)\n    val lemmas = new ArrayBuffer[String]()\n    val sentences = doc.get(classOf[SentencesAnnotation])\n    for (sentence <- sentences; token <- sentence.get(classOf[TokensAnnotation])) {\n        val lemma = token.get(classOf[LemmaAnnotation])\n        val pos = token.get(classOf[PartOfSpeechAnnotation])\n        \n        // get words with length greater that 2 and keep only nouns\n        if (lemma.length > 2 && lemma != \"-lsb-\" && lemma != \"-rsb-\" && lemma != \"-lrb-\" && lemma != \"-rrb-\" && pos.equalsIgnoreCase(\"NN\") || pos.equalsIgnoreCase(\"NNP\")) {\n        lemmas += lemma.toLowerCase\n        }\n    }\n    \n    lemmas.mkString(\" \")\n    \n}.map(Row(_))\n\n// convert RDD to dataframe\nval schema = StructType(Seq(StructField(\"Text\", StringType)))\nval dfLemma = sqlContext.createDataFrame(rddLemma, schema).cache\n\n// stop words removal\nval regexTokenizer = new RegexTokenizer()\n  .setInputCol(\"Text\")\n  .setOutputCol(\"words\")\n  .setPattern(\"[^A-Za-z]+\")\nval dftokenized = regexTokenizer.transform(dfLemma)\nval stopWordsRemover = new StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\")\n\n// cache the cleaned dataframe\nval dfFiltered = stopWordsRemover.transform(dftokenized).select(\"filtered\").cache\nz.show(dfFiltered)\n","dateUpdated":"2017-08-28T22:08:03+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"filtered","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"filtered","index":0,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1503958083586_-315707770","id":"20170821-210651_1795013877","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14532"},{"title":"Most Frequent Words","text":"val dfWords = dfFiltered.withColumn(\"word\", explode($\"filtered\")).select(\"word\").groupBy($\"word\").count.orderBy($\"count\".desc)\n\nz.show(dfWords.limit(10))","dateUpdated":"2017-08-28T22:08:03+0000","config":{"colWidth":9,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"multiBarChart","height":226,"optionOpen":false,"keys":[{"name":"word","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1503958083590_-317246765","id":"20170818-205357_1396664350","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14533"},{"title":"Get Vocab Size","text":"// keeping Nouns reduces the vocab from 114124 to 90066\nval vocabSize = dfWords.select(\"word\").distinct.count","dateUpdated":"2017-08-28T22:08:03+0000","config":{"colWidth":3,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":93,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1503958083590_-317246765","id":"20170818-212834_475834105","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14534"},{"title":"Build Vocab Dictionary","text":"import org.apache.spark.sql.types.StringType\nimport org.apache.spark.ml.feature.StringIndexer\n\nval df_vocab = dfFiltered.withColumn(\"word\", explode($\"filtered\")).select(\"word\")\nval stringindexer = new StringIndexer().setInputCol(\"word\").setOutputCol(\"index\")\nval dfCorpus = stringindexer.fit(df_vocab).transform(df_vocab).distinct().orderBy(\"index\").cache\n\nval dict = dfCorpus.map(x => (x.getDouble(1).toInt, x.getString(0))).collect.toMap\nval broadcastDict = sc.broadcast(dict)\n\nz.show(dfCorpus)","dateUpdated":"2017-08-28T22:08:03+0000","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"word","index":0,"aggr":"sum"}],"values":[{"name":"index","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"word","index":0,"aggr":"sum"},"yAxis":{"name":"index","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1503958083590_-317246765","id":"20170818-221426_1296604024","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14535"},{"title":"Count Vectorization","text":"import org.apache.spark.ml.feature.Word2Vec\nimport org.apache.spark.ml.feature.CountVectorizer\nval countVectorizer = new CountVectorizer()\n                            .setVocabSize(vocabSize.toInt)\n                            .setInputCol(\"filtered\")\n                            .setOutputCol(\"vector\")\n                            .fit(dfFiltered)\n\nval countDF = countVectorizer.transform(dfFiltered).cache\nz.show(countDF.select(\"vector\").limit(200))","dateUpdated":"2017-08-28T22:08:03+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1503958083591_-317631514","id":"20170818-201002_318350591","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14536"},{"title":"Train LDA","text":"import org.apache.spark.ml.clustering.LDA\n\nval ldaModel = new LDA().setK(30).setDocConcentration(.02).setMaxIter(1000).setFeaturesCol(\"vector\")\nval model = ldaModel.fit(countDF)\nval transformed = model.transform(countDF)","dateUpdated":"2017-08-28T22:08:03+0000","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1503958083591_-317631514","id":"20170818-220325_1604937577","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14537"},{"title":"Topic - Words Assignement","text":"import org.apache.spark.sql.functions.udf\n\ndef getWord(s1: Seq[Int]): String = {\n    \n  var words = Seq.empty[String]\n  for (index <- s1)\n  {\n    words = words :+ broadcastDict.value.get(index).toString.replace(\"Some\", \"\")   \n  }\n  \n  words.toArray.mkString(\", \")\n}\n\nval funct = udf(getWord _)\nval topics = model.describeTopics(25)\nz.show(topics.withColumn(\"words\", funct($\"termIndices\")).select(\"topic\", \"words\"))","dateUpdated":"2017-08-28T22:08:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"topic","index":0,"aggr":"sum"}],"values":[{"name":"words","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"topic","index":0,"aggr":"sum"},"yAxis":{"name":"words","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1503958083592_-319555259","id":"20170823-153428_1312918524","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14538"},{"title":"Get Document - Topic Assignment","text":"import org.apache.spark.mllib.linalg.{Vector, Vectors}\nimport org.apache.spark.sql.functions.monotonicallyIncreasingId\nimport org.apache.spark.sql.types.{LongType}\n\nval udfGetTopicId = udf((x: Vector) => x.argmax)\nval udfGetTopicScore = udf((x: Vector) => x(x.argmax))\n\ndef addColumnIndex(df: org.apache.spark.sql.DataFrame) = sqlContext.createDataFrame(\n  // Add Column index\n  df.rdd.zipWithIndex.map{case (row, columnindex) => Row.fromSeq(row.toSeq :+ columnindex)},\n  // Create schema\n  StructType(df.schema.fields :+ StructField(\"Doc_Id\", LongType, false))\n)\n\nval dfTransformed = transformed.select(\"topicDistribution\")\n                    .withColumn(\"Topic_Id\", udfGetTopicId($\"topicDistribution\"))\n                    .withColumn(\"Topic_Score\", udfGetTopicScore($\"topicDistribution\"))\n                    .select(\"Topic_Id\", \"Topic_Score\")\n\nval dfTransformed_Indexed = addColumnIndex(dfTransformed).cache\n\n// combine documents and topics assigned to each document\nval df_Doc_Topic = df_FoodReview.join(dfTransformed_Indexed, \"Doc_Id\").orderBy(\"Doc_Id\").cache\n\nz.show(dfTransformed_Indexed.select(\"Doc_Id\", \"Topic_Id\", \"Topic_Score\", \"Text\"))\n//z.show(dfTransformed.limit(100))","dateUpdated":"2017-08-28T22:08:03+0000","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1503958083592_-319555259","id":"20170821-175656_2110255191","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14539"},{"title":"Build Topic-Aspect Dictionary","text":"import scala.collection.mutable.ListBuffer\nimport org.apache.spark.sql.types.{StringType, ArrayType}\n\n// coffee - 1 \n// sauce and spices - 5\n// pharmacy - 7\n// service - 26\n// family - 14\n// Drinks - 15\n// Skin Products - 17\n// Snacks - 18\n// Candies and Gums - 20\n// Tea - 25\n// Gifts - 28\n\n// drinks, service, skinproducts, snacks, \n// mapTopicAspect += \"AnimalFood\" -> ListBuffer[String](\"cat\", \"tuna\", \"salmon\", \"fish\", \"kitty\", \"dog\", \"bone\")\n\nval mapTopicAspect = scala.collection.mutable.Map[String, (ListBuffer[String], Long)]()\nmapTopicAspect += \"Product\" -> (ListBuffer[String](\"product\", \"food\", \"tea\", \"coffee\", \"chai\", \"water\", \"espresso\", \"cappucino\", \"juice\", \"wine\", \"beer\", \"soda\", \"cola\", \"coke\", \"tea\", \"beverage\", \"syrup\", \"milk\", \"snacks\", \"chocolate\", \"cereal\", \"granola\", \"wafer\", \"cookie\", \"bar\", \"cereal\", \"nut\", \"soup\", \"candy\", \"chips\", \"biscuit\", \"oatmeal\", \"treats\", \"oil\", \"shampoo\", \"conditioner\", \"scent\", \"lipton\", \"pepsi\", \"snack\", \"saffron\", \"cashews\", \"biscuits\", \"seafood\"), -1)\nmapTopicAspect += \"Service\" -> (ListBuffer[String](\"amazon\", \"price\", \"order\", \"ordering\", \"online\", \"schedule\", \"service\", \"shipping\", \"delivery\", \"customer\", \"seller\", \"purchase\", \"condition\", \"deal\", \"product\" ,\"item\" , \"shipment\", \"expiration\", \"time\", \"quality\", \"review\", \"website\"), 26)\nmapTopicAspect += \"Coffee\" -> (ListBuffer[String](\"coffee\", \"roast\", \"espresso\", \"starbucks\", \"decaf\", \"aroma\", \"hazelnut\", \"blend\", \"cappucino\"), 0)\nmapTopicAspect += \"Drinks\" -> (ListBuffer[String](\"water\", \"drink\", \"juice\", \"wine\", \"beer\", \"coffee\", \"soda\", \"cola\", \"coke\", \"tea\", \"beverage\", \"bottle\", \"syrup\", \"milk\"), 15)\nmapTopicAspect += \"Snacks\" -> (ListBuffer[String](\"snack\", \"chocolate\", \"cereal\", \"granola\", \"wafer\", \"cookie\", \"bar\", \"cereal\", \"nut\", \"soup\", \"candy\", \"chips\", \"biscuits\", \"oatmeal\", \"treats\", \"cashews\"), 18)\nmapTopicAspect += \"Skincare\" -> (ListBuffer[String](\"oil\", \"hair\", \"skin\", \"shampoo\", \"conditioner\", \"scalp\", \"scent\", \"body\"), 17)\nmapTopicAspect += \"Tea\" -> (ListBuffer[String](\"tea\", \"mint\", \"chai\", \"peppermint\", \"leaf\", \"green\", \"earl\", \"grey\", \"matcha\", \"lipton\", \"english\"), 25)\nmapTopicAspect += \"Seafood\" -> (ListBuffer[String](\"Seafood\", \"tuna\", \"salmon\", \"fish\"), 25)\n\n//val topic_schema = StructType(Seq(StructField(\"Topic\", StringType), StructField(\"Aspects\", ArrayType(StringType))))\nval initialColumnNames = Seq(\"Aspect\", \"Terms\", \"Topic_Index\")\nval dfTopics = sqlContext.createDataFrame(sc.parallelize(mapTopicAspect.toSeq).map{case x =>  (x._1, x._2._1.mkString(\", \"), x._2._2)}).toDF(initialColumnNames: _*)\nz.show(dfTopics)\n//mapTopicAspect","dateUpdated":"2017-08-28T22:08:03+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"Aspect","index":0,"aggr":"sum"}],"values":[{"name":"Terms","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"Aspect","index":0,"aggr":"sum"},"yAxis":{"name":"Terms","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1503958083592_-319555259","id":"20170823-161951_578750703","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14540"},{"title":"Define CoreNLPFuntions","text":"import scala.collection.mutable._\nimport math._\n\n// get sentiment for label and entire string\n  def getSentimentForLabel(line: String, aspect: String): (ListBuffer[Int], ListBuffer[Double] ,ListBuffer[Int], ListBuffer[Double]) = {\n      \n    import java.util.Properties\n    import edu.stanford.nlp.ling.CoreAnnotations\n    import edu.stanford.nlp.neural.rnn.RNNCoreAnnotations\n    import edu.stanford.nlp.pipeline.Annotation\n    import edu.stanford.nlp.pipeline.StanfordCoreNLP\n    import edu.stanford.nlp.sentiment.SentimentCoreAnnotations\n    import edu.stanford.nlp.trees.Tree\n    import edu.stanford.nlp.util.CoreMap\n    import scala.util.control._\n    import scala.collection.JavaConversions._\n    import util.control.Breaks._\n    import edu.stanford.nlp.ling._\n    import scala.collection.mutable._\n    import org.apache.spark.sql.functions.udf\n\n    val lstMainSentiment = scala.collection.mutable.ListBuffer.empty[Int]\n    val lstMainScore = scala.collection.mutable.ListBuffer.empty[Double]\n    val lstAspectSentiment = scala.collection.mutable.ListBuffer.empty[Int]\n    val lstAspectScore = scala.collection.mutable.ListBuffer.empty[Double]\n    \n    val props = new Properties();\n    props.setProperty(\"annotators\", \"tokenize, ssplit, parse, sentiment\");\n    val nlp = new StanfordCoreNLP(props);\n    \n    if (line != null && line.length > 0) {\n      val annotation = nlp.process(line)\n\n      //breakable {\n        // loop through every sentence\n        for (sentence <- annotation.get(classOf[CoreAnnotations.SentencesAnnotation])) {\n\n          // get root tree, its sentiment and score\n          val tree = sentence.get(classOf[SentimentCoreAnnotations.SentimentAnnotatedTree])\n          var mainSentiment = RNNCoreAnnotations.getPredictedClass(tree);\n          lstMainSentiment += mainSentiment;\n          lstMainScore  += tree.label().asInstanceOf[edu.stanford.nlp.ling.CoreLabel].get(classOf[RNNCoreAnnotations.Predictions]).get(mainSentiment);\n\n          // set variables\n          var aspectScore = -1.0; // negative if aspect not found\n          var aspectIndex = 0 // starting index to find aspect\n          var minSentiment = 2  // neutral\n\n          // get all leaf nodes\n          val leaves = tree.getLeaves()\n          //println(leaves)\n\n          breakable {\n\n            // loop through leaf nodes\n            for (leave <- leaves.toArray) {\n\n              // check if leaf node matches aspect\n              if (leave.toString().equalsIgnoreCase(aspect)) {\n\n                // get tree of aspect\n                var aspectTree: edu.stanford.nlp.trees.LabeledScoredTreeNode = tree.getLeaves()(aspectIndex)\n\n                // loop through every parent of aspect until we find a non-neutral node\n                while (!aspectTree.label().toString().equalsIgnoreCase(\"ROOT\") && minSentiment == 2) {\n\n                  // reset aspect score\n                  aspectScore = 0.2\n\n                  // get parent\n                  aspectTree = aspectTree.parent(tree).asInstanceOf[edu.stanford.nlp.trees.LabeledScoredTreeNode];\n\n                  // get matrix of scores\n                  val leafNodeSentimentMatrix = aspectTree.label().asInstanceOf[edu.stanford.nlp.ling.CoreLabel].get(classOf[RNNCoreAnnotations.Predictions])\n\n                  // set aspect score and index to the max value in matrix\n                  for (index <- 0 until 5) {\n                    if (aspectScore < leafNodeSentimentMatrix.get(index)) {\n                      aspectScore = leafNodeSentimentMatrix.get(index)\n                      minSentiment = index\n                    }\n                  }\n                }\n\n                // break from inner loop if aspect is found\n                break;\n              }\n              // move to next leaf node\n              aspectIndex = aspectIndex + 1\n            }\n          }\n\n          // set final aspect sentiment\n          lstAspectSentiment += minSentiment\n          lstAspectScore += aspectScore\n        }\n    }\n\n    // return values\n    (lstMainSentiment, lstMainScore, lstAspectSentiment, lstAspectScore);\n  }\n\n// return sentiment for sentiment score\ndef getSentimentString(sentiment: Double): String = {\n    \n    var value = sentiment \n    \n    // there is alteast one negative sentiment\n    if (sentiment < 2.0)\n    {\n        value = floor(sentiment)\n    }\n    \n    // there is alteast one postive sentiment\n    if (sentiment > 2.0)\n    {\n        value = ceil(sentiment)\n    }\n    \n    value.toInt match\n    {\n        case 0 => \"negative\"\n        case 1 => \"negative\"\n        case 2 => \"neutral\"\n        case 3 => \"positive\"\n        case 4 => \"positive\"\n    }\n}\n\n// get sentiment for the entire row\ndef getOverallSentiment(line: String): String = {\n    \n   val (lstMainSentiment, lstMainScore, lstAspectSentiment, lstAspectScore) = getSentimentForLabel(line, \"\")\n   \n   var sentiment = 0.0\n   \n   // for each sentence is the row of sentences, get sentiment on sentence (0, 1, 2, 3, 4)\n   for (s <- lstMainSentiment)\n   {\n       sentiment = sentiment + s\n   }\n   \n   // average the sentiment to get the overall sentiment on the row\n   sentiment = sentiment/lstMainSentiment.length\n   println(sentiment)\n   getSentimentString(sentiment)\n}\n\n\n// get sentiment score for the entire row\ndef getOverallScore(line: String): Double = {\n    \n   val (lstMainSentiment, lstMainScore, lstAspectSentiment, lstAspectScore) = getSentimentForLabel(line, \"\")\n   \n   var score = 0.0\n   \n   // for each sentence is the row of sentences, get score\n   for (s <- lstMainScore)\n   {\n       score = score + s\n   }\n   \n   // average the score to get the overall score on the row. The sentiment for this score is given by method getOverallSentiment\n   score = score/lstMainScore.length\n   return score\n}\n\n// get sentiment for an aspec in the entire row\ndef getOverall_Aspect_Sentiment(line: String, aspect: String): (String, String) = {\n   \n   // getSentimentForLabel returns -1 if the apect is not present in the sentence \n   val (lstMainSentiment, lstMainScore, lstAspectSentiment, lstAspectScore) = getSentimentForLabel(line, aspect)\n   \n   var sentiment = 0.0\n   var aspectOccuranceCount = 0\n   var over_sentiment = 0.0\n   var aspect_sentiment_string = \"\"\n   var over_sentiment_string = \"\"\n   \n   // get aspect sentiment\n   for ((s, j) <- lstAspectSentiment zip lstAspectScore)\n   {\n       // only add if sentiment is present (non negative value for score)\n       if(j != -1.0)\n       {\n           sentiment = sentiment + s\n           aspectOccuranceCount = aspectOccuranceCount + 1\n       }\n   }\n   \n   // return the sentiment if aspect occur else return \"aspect not found\"\n   if(aspectOccuranceCount != 0)\n   {\n        sentiment = sentiment/aspectOccuranceCount\n        aspect_sentiment_string = getSentimentString(sentiment)\n   }else\n   {\n       aspect_sentiment_string = \"aspect not found\"\n   }\n   \n    \n    // get overall sentiment\n    // for each sentence is the row of sentences, get sentiment on sentence (0, 1, 2, 3, 4)\n   for (s <- lstMainSentiment)\n   {\n       over_sentiment = over_sentiment + s\n   }\n   \n   // average the sentiment to get the overall sentiment on the row\n   over_sentiment = over_sentiment/lstMainSentiment.length\n   over_sentiment_string = getSentimentString(over_sentiment)\n   \n   return (over_sentiment_string, aspect_sentiment_string)\n}\n\n// get sentiment for an aspec in the entire row\ndef getAspectSentiment(line: String, aspect: String): String = {\n   \n   // getSentimentForLabel returns -1 if the apect is not present in the sentence \n   val (lstMainSentiment, lstMainScore, lstAspectSentiment, lstAspectScore) = getSentimentForLabel(line, aspect)\n   \n   var sentiment = 0.0\n   var aspectOccuranceCount = 0\n   \n   for ((s, j) <- lstAspectSentiment zip lstAspectScore)\n   {\n       // only add if sentiment is present (non negative value for score)\n       if(j != -1.0)\n       {\n           sentiment = sentiment + s\n           aspectOccuranceCount = aspectOccuranceCount + 1\n       }\n   }\n   \n   // return the sentiment if aspect occur else return \"aspect not found\"\n   if(aspectOccuranceCount != 0)\n   {\n        sentiment = sentiment/aspectOccuranceCount\n        return getSentimentString(sentiment)\n   }else\n   {\n       return \"aspect not found\"\n   }\n}\n\ndef getAspectScore(line: String, aspect: String): Double = {\n\n   // getSentimentForLabel returns -1 if the apect is not present in the sentence    \n   val (lstMainSentiment, lstMainScore, lstAspectSentiment, lstAspectScore) = getSentimentForLabel(line, aspect)\n   \n   var score = 0.0\n   var aspectOccuranceCount = 0\n   \n   for (s <- lstAspectScore)\n   {\n        // only add if sentiment is present\n        if(s != -1.0)\n        {\n            score = score + s\n            aspectOccuranceCount = aspectOccuranceCount + 1\n       }\n   }\n   \n    // return the score if aspect occur. Zero aspect socre means aspect is not present in the row.\n    if(aspectOccuranceCount != 0)\n    {\n        score = score/aspectOccuranceCount\n        return score\n    }else\n    {\n        return 0.0\n    }\n}\n\n// udf to get aspect sentiment to get aspe\nval udfAspectBasedSentiment = udf(getAspectSentiment(_:String, _:String))","dateUpdated":"2017-08-28T22:08:03+0000","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1503958083592_-319555259","id":"20170823-162425_1906968061","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14541"},{"title":"Load Small Dataset","text":"val df_limit_foodReview = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"amazon_food.csv\")\nz.show(df_limit_foodReview)","dateUpdated":"2017-08-28T22:08:03+0000","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":408,"optionOpen":false,"keys":[{"name":"Text","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"Text","index":0,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1503958083592_-319555259","id":"20170825-151429_763442745","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14542"},{"title":"Sentiment Analysis","text":"val schema = StructType(Seq(StructField(\"text\",StringType) , StructField(\"Text_Sentiment\",StringType)))\n\nval df_Sentiment_Text = sqlContext.createDataFrame(df_limit_foodReview.map{ r => \n\n    Row(r(0).toString, getOverallSentiment(r(0).toString))\n    \n}, schema).cache\n\ndf_Sentiment_Text.count\nz.run(\"20170828-210640_1377783392\")\nz.show(df_Sentiment_Text)","dateUpdated":"2017-08-28T22:08:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"text","index":0,"aggr":"sum"}],"values":[{"name":"Text_Sentiment","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"text","index":0,"aggr":"sum"},"yAxis":{"name":"Text_Sentiment","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{"Enter Aspect":""},"forms":{}},"jobName":"paragraph_1503958083592_-319555259","id":"20170828-210436_1674755903","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14543"},{"text":"z.show(df_Sentiment_Text)","dateUpdated":"2017-08-28T22:08:03+0000","config":{"colWidth":12,"editorHide":true,"graph":{"mode":"pieChart","height":300,"optionOpen":false,"keys":[{"name":"Text_Sentiment","index":1,"aggr":"sum"}],"values":[{"name":"Text_Sentiment","index":1,"aggr":"count"}],"groups":[],"scatter":{"xAxis":{"name":"text","index":0,"aggr":"sum"},"yAxis":{"name":"Text_Sentiment","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1503958083592_-319555259","id":"20170828-210640_1377783392","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14544"},{"title":"Aspect Based Sentiment Analysis","text":"val topic = z.input(\"Enter Aspect\")\n\nval schema = StructType(Seq(StructField(\"text\",StringType) , StructField(\"Entity_Sentiment\",StringType), StructField(\"Aspect_Sentiment\",StringType)))\n\nval df_Sentiment = sqlContext.createDataFrame(df_limit_foodReview.map{ r => \n\nvar positive = 0L\nvar negative = 0L\nvar neutral = 0L\nvar topic_sentiment = \"None\"\nvar aspect_sentiment = Map[String, String]()\n\nfor (aspect <- mapTopicAspect(topic.asInstanceOf[String])._1)\n{\n    getAspectSentiment(r(0).toString, aspect) match \n    {\n        case \"positive\" => positive = positive + 1; aspect_sentiment += aspect -> \"positive\"\n        case \"negative\" => negative = negative + 1; aspect_sentiment += aspect -> \"negative\"\n        case \"neutral\" => neutral = neutral + 1; aspect_sentiment += aspect -> \"neutral\"\n        case \"aspect not found\" => \n    }\n}\n    \n    if(positive > negative && positive > neutral)\n    {\n        topic_sentiment = \"Positive\"\n    }\n    \n    if(negative > positive && negative > neutral)\n    {\n        topic_sentiment = \"Negative\"\n    }\n    \n    if(neutral > positive && neutral > negative)\n    {\n        topic_sentiment = \"Neutral\"\n    }\n    \n    Row(r(0).toString, aspect_sentiment.toString.replace(\"Map(\", \"\").replace(\")\", \"\"), topic_sentiment)\n}, schema).cache\n\ndf_Sentiment.count\nz.show(df_Sentiment)\nz.run(\"20170825-153701_1443237643\")","dateUpdated":"2017-08-28T22:08:03+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"Enter Aspect":"Coffee","aspect":"food"},"forms":{"Enter Aspect":{"name":"Enter Aspect","displayName":"Enter Aspect","type":"input","defaultValue":"","hidden":false}}},"jobName":"paragraph_1503958083592_-319555259","id":"20170817-193615_189059607","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14545"},{"title":"Aspect Sentiment","text":"z.show(df_Sentiment.filter(\"Aspect_Sentiment != 'None'\"))","dateUpdated":"2017-08-28T22:08:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"pieChart","height":300,"optionOpen":false,"keys":[{"name":"Aspect_Sentiment","index":2,"aggr":"sum"}],"values":[{"name":"Aspect_Sentiment","index":2,"aggr":"count"}],"groups":[],"scatter":{"xAxis":{"name":"text","index":0,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1503958083592_-319555259","id":"20170825-153701_1443237643","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14546"},{"dateUpdated":"2017-08-28T22:08:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1503958083592_-319555259","id":"20170825-153737_1403152073","dateCreated":"2017-08-28T22:08:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14547"}],"name":"LDA_Sentiment_Analysis","id":"2CQVZ31EQ","angularObjects":{"2CSK4631B:shared_process":[],"2CQECCVSU:shared_process":[],"2CR848DW4:shared_process":[],"2CQP88XBU:shared_process":[],"2CQTGB6U5:shared_process":[],"2CS3BY5RH:shared_process":[],"2CRF55T8Z:shared_process":[],"2CSS1T5JT:shared_process":[],"2CRYS1197:shared_process":[],"2CPY7XMQN:shared_process":[],"2CPUKP3QX:shared_process":[],"2CT9X7PFQ:shared_process":[],"2CT67D9MP:shared_process":[],"2CS3Y6AXN:shared_process":[],"2CRKSUPR1:shared_process":[],"2CSSV4ZJG:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}